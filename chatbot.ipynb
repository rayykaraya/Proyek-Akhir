{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb02142",
   "metadata": {},
   "source": [
    "# Ekstrak PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86076479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "ISSN 2302-4283 (print) \n",
      " \n",
      "ISSN 2580-9571 (online) \n",
      " \n",
      "Online di https://jurnal.poltekkes-soepraoen.ac.id \n",
      " \n",
      "DOI: 10.47794/jkhws \n",
      "10 | Jurnal Kesehatan Hesti Wira Sakti; Volume 12 Nomor 01, April 2024 \n",
      " \n",
      " \n",
      "ETIKA DALAM ASURANSI KESEHATAN: MENEMUKAN \n",
      "KESEIMBANGAN DAN KEADILAN  \n",
      "Annisya Putri Salsabila1, Putri Naira Kusuma2, Riswandy Wasir3, Cahya Arbitera4  \n",
      "1,2,3,4Universitas Pembangunan Nasional Veteran Jakarta  \n",
      "(Korespondensi: 2110713100@mahasiswa.upnvj.ac.id)  \n",
      " \n",
      "ABSTRAK   \n",
      "Pendahuluan: Etika merupakan komponen penting dalam berbisnis. Dalam dunia asuransi kesehatan, \n",
      "etika diperlukan untuk menunjang keadilan dan keseimbangan bagi perusahaan dan konsumen. \n",
      "Perusahaan wajib memenuhi hak-hak konsumen secara penuh, namun masih banyak perusahaan yang \n",
      "mementingkan keuntungan semata dengan melakukan kecurangan atau fraud. Melalui penelitian ini \n",
      "diharapkan memberikan wawasan mendalam tentang masalah etika dan membantu perusahaan asuransi \n",
      "kesehatan menjalankan bisnis mereka dengan cara\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Folder tempat file PDF kamu\n",
    "folder_path = \"PDF\"\n",
    "\n",
    "# Simpan hasil ekstraksi dalam list\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        data.append({\n",
    "            \"filename\": filename,\n",
    "            \"konten\": text\n",
    "        })\n",
    "\n",
    "# Simpan semua ke DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Contoh tampilkan 1 artikel\n",
    "print(df.iloc[0]['konten'][:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc5fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"question\": \"Apa tujuan dari asuransi?\",\n",
    "        \"context\": \"Asuransi bertujuan untuk memberikan perlindungan finansial terhadap risiko yang tidak terduga.\",\n",
    "        \"answers\": {\"text\": [\"memberikan perlindungan finansial\"], \"answer_start\": [28]}\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Apa yang dimaksud dengan premi?\",\n",
    "        \"context\": \"Premi adalah sejumlah uang yang dibayarkan oleh pemegang polis kepada perusahaan asuransi.\",\n",
    "        \"answers\": {\"text\": [\"sejumlah uang\"], \"answer_start\": [10]}\n",
    "    }\n",
    "]\n",
    "\n",
    "import json\n",
    "with open(\"qa_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0dbd721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 29.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    \n",
    "    answers = example[\"answers\"]\n",
    "    start_char = answers[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    context = example[\"context\"]\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids_sample = input_ids[i]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find start of context\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "\n",
    "        # Only keep offsets that belong to the context\n",
    "        offsets_context = offsets[context_start:context_end]\n",
    "\n",
    "        start = end = 0\n",
    "        for idx, (start_off, end_off) in enumerate(offsets_context):\n",
    "            if start_off <= start_char < end_off:\n",
    "                start = context_start + idx\n",
    "            if start_off < end_char <= end_off:\n",
    "                end = context_start + idx\n",
    "                break\n",
    "        \n",
    "        start_positions.append(start)\n",
    "        end_positions.append(end)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72286c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa_model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3b26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": tokenized_dataset\n",
    "})\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "980da03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=trainer.model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ced3094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Asuransi kesehatan adalah bentuk perlindungan finansial terhadap biaya pengobatan.\n",
    "Premi dibayarkan secara rutin kepada penyedia asuransi untuk mendapatkan manfaat perlindungan tersebut.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a852afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Apa itu asuransi kesehatan?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2291102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\T480\\Documents\\CAPSTONE\\fixenv\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0003993232094217092, 'start': 108, 'end': 138, 'answer': 'rutin kepada penyedia asuransi'}\n"
     ]
    }
   ],
   "source": [
    "result = qa_pipeline({\n",
    "    \"context\": context,\n",
    "    \"question\": question\n",
    "})\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
